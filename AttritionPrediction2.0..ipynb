{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Francky46/AttritionPrediction/blob/main/AttritionPrediction2.0..ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "id": "6e2196539be3fed8"
  },
  {
   "metadata": {
    "id": "124bbaf532a0cd66"
   },
   "cell_type": "markdown",
   "source": [
    "# 1) Importation des librairies"
   ],
   "id": "124bbaf532a0cd66"
  },
  {
   "metadata": {
    "id": "45d03adf497a0cdf",
    "ExecuteTime": {
     "end_time": "2025-01-30T10:44:52.056366Z",
     "start_time": "2025-01-30T10:44:52.053602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from cProfile import label\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pour la séparation des données en train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pour la création et l'évaluation de modèles\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "\n",
    "# Pour l'optimisation des hyperparamètres\n",
    "from sklearn.model_selection import GridSearchCV"
   ],
   "id": "45d03adf497a0cdf",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "id": "59d618d81bb602ef"
   },
   "cell_type": "markdown",
   "source": [
    "# 2) Chargement des données"
   ],
   "id": "59d618d81bb602ef"
  },
  {
   "metadata": {
    "id": "71cf01ea2425e2b1",
    "outputId": "fd95ad87-8e8e-4ec2-e855-934119149fa9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "ExecuteTime": {
     "end_time": "2025-01-30T10:44:53.458068Z",
     "start_time": "2025-01-30T10:44:52.070002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Lis les différents fichiers CSV\n",
    "folder = \"src/\"\n",
    "df_general = pd.read_csv(folder + \"general_data.csv\")\n",
    "df_emp_survey = pd.read_csv(folder + \"employee_survey_data.csv\")\n",
    "df_mgr_survey = pd.read_csv(folder + \"manager_survey_data.csv\")\n",
    "df_in_time = pd.read_csv(folder + \"in_time.csv\")\n",
    "df_out_time = pd.read_csv(folder + \"out_time.csv\")"
   ],
   "id": "71cf01ea2425e2b1",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "id": "7b1dc6ecee341936"
   },
   "cell_type": "markdown",
   "source": [
    "# 3) Exploration des données"
   ],
   "id": "7b1dc6ecee341936"
  },
  {
   "metadata": {
    "id": "7ea3ad16748652c9",
    "ExecuteTime": {
     "end_time": "2025-01-30T10:44:53.674069Z",
     "start_time": "2025-01-30T10:44:53.608831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=== General Data ===\")\n",
    "print(df_general.head(), \"\\n\")\n",
    "print(df_general.info(), \"\\n\")\n",
    "\n",
    "print(\"=== Employee Survey Data ===\")\n",
    "print(df_emp_survey.head(), \"\\n\")\n",
    "print(df_emp_survey.info(), \"\\n\")\n",
    "\n",
    "print(\"=== Manager Survey Data ===\")\n",
    "print(df_mgr_survey.head(), \"\\n\")\n",
    "print(df_mgr_survey.info(), \"\\n\")\n",
    "\n",
    "print(\"=== In Time Data ===\")\n",
    "print(df_in_time.head(), \"\\n\")\n",
    "print(df_in_time.info(), \"\\n\")\n",
    "\n",
    "print(\"=== Out Time Data ===\")\n",
    "print(df_out_time.head(), \"\\n\")\n",
    "print(df_out_time.info(), \"\\n\")"
   ],
   "id": "7ea3ad16748652c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== General Data ===\n",
      "   Age Attrition     BusinessTravel              Department  DistanceFromHome  \\\n",
      "0   51        No      Travel_Rarely                   Sales                 6   \n",
      "1   31       Yes  Travel_Frequently  Research & Development                10   \n",
      "2   32        No  Travel_Frequently  Research & Development                17   \n",
      "3   38        No         Non-Travel  Research & Development                 2   \n",
      "4   32        No      Travel_Rarely  Research & Development                10   \n",
      "\n",
      "   Education EducationField  EmployeeCount  EmployeeID  Gender  ...  \\\n",
      "0          2  Life Sciences              1           1  Female  ...   \n",
      "1          1  Life Sciences              1           2  Female  ...   \n",
      "2          4          Other              1           3    Male  ...   \n",
      "3          5  Life Sciences              1           4    Male  ...   \n",
      "4          1        Medical              1           5    Male  ...   \n",
      "\n",
      "   NumCompaniesWorked Over18 PercentSalaryHike  StandardHours  \\\n",
      "0                 1.0      Y                11              8   \n",
      "1                 0.0      Y                23              8   \n",
      "2                 1.0      Y                15              8   \n",
      "3                 3.0      Y                11              8   \n",
      "4                 4.0      Y                12              8   \n",
      "\n",
      "   StockOptionLevel TotalWorkingYears  TrainingTimesLastYear  YearsAtCompany  \\\n",
      "0                 0               1.0                      6               1   \n",
      "1                 1               6.0                      3               5   \n",
      "2                 3               5.0                      2               5   \n",
      "3                 3              13.0                      5               8   \n",
      "4                 2               9.0                      2               6   \n",
      "\n",
      "   YearsSinceLastPromotion  YearsWithCurrManager  \n",
      "0                        0                     0  \n",
      "1                        1                     4  \n",
      "2                        0                     3  \n",
      "3                        7                     5  \n",
      "4                        0                     4  \n",
      "\n",
      "[5 rows x 24 columns] \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4410 entries, 0 to 4409\n",
      "Data columns (total 24 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Age                      4410 non-null   int64  \n",
      " 1   Attrition                4410 non-null   object \n",
      " 2   BusinessTravel           4410 non-null   object \n",
      " 3   Department               4410 non-null   object \n",
      " 4   DistanceFromHome         4410 non-null   int64  \n",
      " 5   Education                4410 non-null   int64  \n",
      " 6   EducationField           4410 non-null   object \n",
      " 7   EmployeeCount            4410 non-null   int64  \n",
      " 8   EmployeeID               4410 non-null   int64  \n",
      " 9   Gender                   4410 non-null   object \n",
      " 10  JobLevel                 4410 non-null   int64  \n",
      " 11  JobRole                  4410 non-null   object \n",
      " 12  MaritalStatus            4410 non-null   object \n",
      " 13  MonthlyIncome            4410 non-null   int64  \n",
      " 14  NumCompaniesWorked       4391 non-null   float64\n",
      " 15  Over18                   4410 non-null   object \n",
      " 16  PercentSalaryHike        4410 non-null   int64  \n",
      " 17  StandardHours            4410 non-null   int64  \n",
      " 18  StockOptionLevel         4410 non-null   int64  \n",
      " 19  TotalWorkingYears        4401 non-null   float64\n",
      " 20  TrainingTimesLastYear    4410 non-null   int64  \n",
      " 21  YearsAtCompany           4410 non-null   int64  \n",
      " 22  YearsSinceLastPromotion  4410 non-null   int64  \n",
      " 23  YearsWithCurrManager     4410 non-null   int64  \n",
      "dtypes: float64(2), int64(14), object(8)\n",
      "memory usage: 827.0+ KB\n",
      "None \n",
      "\n",
      "=== Employee Survey Data ===\n",
      "   EmployeeID  EnvironmentSatisfaction  JobSatisfaction  WorkLifeBalance\n",
      "0           1                      3.0              4.0              2.0\n",
      "1           2                      3.0              2.0              4.0\n",
      "2           3                      2.0              2.0              1.0\n",
      "3           4                      4.0              4.0              3.0\n",
      "4           5                      4.0              1.0              3.0 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4410 entries, 0 to 4409\n",
      "Data columns (total 4 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   EmployeeID               4410 non-null   int64  \n",
      " 1   EnvironmentSatisfaction  4385 non-null   float64\n",
      " 2   JobSatisfaction          4390 non-null   float64\n",
      " 3   WorkLifeBalance          4372 non-null   float64\n",
      "dtypes: float64(3), int64(1)\n",
      "memory usage: 137.9 KB\n",
      "None \n",
      "\n",
      "=== Manager Survey Data ===\n",
      "   EmployeeID  JobInvolvement  PerformanceRating\n",
      "0           1               3                  3\n",
      "1           2               2                  4\n",
      "2           3               3                  3\n",
      "3           4               2                  3\n",
      "4           5               3                  3 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4410 entries, 0 to 4409\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype\n",
      "---  ------             --------------  -----\n",
      " 0   EmployeeID         4410 non-null   int64\n",
      " 1   JobInvolvement     4410 non-null   int64\n",
      " 2   PerformanceRating  4410 non-null   int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 103.5 KB\n",
      "None \n",
      "\n",
      "=== In Time Data ===\n",
      "   Unnamed: 0  2015-01-01           2015-01-02           2015-01-05  \\\n",
      "0           1         NaN  2015-01-02 09:43:45  2015-01-05 10:08:48   \n",
      "1           2         NaN  2015-01-02 10:15:44  2015-01-05 10:21:05   \n",
      "2           3         NaN  2015-01-02 10:17:41  2015-01-05 09:50:50   \n",
      "3           4         NaN  2015-01-02 10:05:06  2015-01-05 09:56:32   \n",
      "4           5         NaN  2015-01-02 10:28:17  2015-01-05 09:49:58   \n",
      "\n",
      "            2015-01-06           2015-01-07           2015-01-08  \\\n",
      "0  2015-01-06 09:54:26  2015-01-07 09:34:31  2015-01-08 09:51:09   \n",
      "1                  NaN  2015-01-07 09:45:17  2015-01-08 10:09:04   \n",
      "2  2015-01-06 10:14:13  2015-01-07 09:47:27  2015-01-08 10:03:40   \n",
      "3  2015-01-06 10:11:07  2015-01-07 09:37:30  2015-01-08 10:02:08   \n",
      "4  2015-01-06 09:45:28  2015-01-07 09:49:37  2015-01-08 10:19:44   \n",
      "\n",
      "            2015-01-09           2015-01-12           2015-01-13  ...  \\\n",
      "0  2015-01-09 10:09:25  2015-01-12 09:42:53  2015-01-13 10:13:06  ...   \n",
      "1  2015-01-09 09:43:26  2015-01-12 10:00:07  2015-01-13 10:43:29  ...   \n",
      "2  2015-01-09 10:05:49  2015-01-12 10:03:47  2015-01-13 10:21:26  ...   \n",
      "3  2015-01-09 10:08:12  2015-01-12 10:13:42  2015-01-13 09:53:22  ...   \n",
      "4  2015-01-09 10:00:50  2015-01-12 10:29:27  2015-01-13 09:59:32  ...   \n",
      "\n",
      "            2015-12-18           2015-12-21           2015-12-22  \\\n",
      "0                  NaN  2015-12-21 09:55:29  2015-12-22 10:04:06   \n",
      "1  2015-12-18 10:37:17  2015-12-21 09:49:02  2015-12-22 10:33:51   \n",
      "2  2015-12-18 10:15:14  2015-12-21 10:10:28  2015-12-22 09:44:44   \n",
      "3  2015-12-18 10:17:38  2015-12-21 09:58:21  2015-12-22 10:04:25   \n",
      "4  2015-12-18 09:58:35  2015-12-21 10:03:41  2015-12-22 10:10:30   \n",
      "\n",
      "            2015-12-23           2015-12-24 2015-12-25           2015-12-28  \\\n",
      "0  2015-12-23 10:14:27  2015-12-24 10:11:35        NaN  2015-12-28 10:13:41   \n",
      "1  2015-12-23 10:12:10                  NaN        NaN  2015-12-28 09:31:45   \n",
      "2  2015-12-23 10:15:54  2015-12-24 10:07:26        NaN  2015-12-28 09:42:05   \n",
      "3  2015-12-23 10:11:46  2015-12-24 09:43:15        NaN  2015-12-28 09:52:44   \n",
      "4  2015-12-23 10:13:36  2015-12-24 09:44:24        NaN  2015-12-28 10:05:15   \n",
      "\n",
      "            2015-12-29           2015-12-30           2015-12-31  \n",
      "0  2015-12-29 10:03:36  2015-12-30 09:54:12  2015-12-31 10:12:44  \n",
      "1  2015-12-29 09:55:49  2015-12-30 10:32:25  2015-12-31 09:27:20  \n",
      "2  2015-12-29 09:43:36  2015-12-30 09:34:05  2015-12-31 10:28:39  \n",
      "3  2015-12-29 09:33:16  2015-12-30 10:18:12  2015-12-31 10:01:15  \n",
      "4  2015-12-29 10:30:53  2015-12-30 09:18:21  2015-12-31 09:41:09  \n",
      "\n",
      "[5 rows x 262 columns] \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4410 entries, 0 to 4409\n",
      "Columns: 262 entries, Unnamed: 0 to 2015-12-31\n",
      "dtypes: float64(12), int64(1), object(249)\n",
      "memory usage: 8.8+ MB\n",
      "None \n",
      "\n",
      "=== Out Time Data ===\n",
      "   Unnamed: 0  2015-01-01           2015-01-02           2015-01-05  \\\n",
      "0           1         NaN  2015-01-02 16:56:15  2015-01-05 17:20:11   \n",
      "1           2         NaN  2015-01-02 18:22:17  2015-01-05 17:48:22   \n",
      "2           3         NaN  2015-01-02 16:59:14  2015-01-05 17:06:46   \n",
      "3           4         NaN  2015-01-02 17:25:24  2015-01-05 17:14:03   \n",
      "4           5         NaN  2015-01-02 18:31:37  2015-01-05 17:49:15   \n",
      "\n",
      "            2015-01-06           2015-01-07           2015-01-08  \\\n",
      "0  2015-01-06 17:19:05  2015-01-07 16:34:55  2015-01-08 17:08:32   \n",
      "1                  NaN  2015-01-07 17:09:06  2015-01-08 17:34:04   \n",
      "2  2015-01-06 16:38:32  2015-01-07 16:33:21  2015-01-08 17:24:22   \n",
      "3  2015-01-06 17:07:42  2015-01-07 16:32:40  2015-01-08 16:53:11   \n",
      "4  2015-01-06 17:26:25  2015-01-07 17:37:59  2015-01-08 17:59:28   \n",
      "\n",
      "            2015-01-09           2015-01-12           2015-01-13  ...  \\\n",
      "0  2015-01-09 17:38:29  2015-01-12 16:58:39  2015-01-13 18:02:58  ...   \n",
      "1  2015-01-09 16:52:29  2015-01-12 17:36:48  2015-01-13 18:00:13  ...   \n",
      "2  2015-01-09 16:57:30  2015-01-12 17:28:54  2015-01-13 17:21:25  ...   \n",
      "3  2015-01-09 17:19:47  2015-01-12 17:13:37  2015-01-13 17:11:45  ...   \n",
      "4  2015-01-09 17:44:08  2015-01-12 18:51:21  2015-01-13 18:14:58  ...   \n",
      "\n",
      "            2015-12-18           2015-12-21           2015-12-22  \\\n",
      "0                  NaN  2015-12-21 17:15:50  2015-12-22 17:27:51   \n",
      "1  2015-12-18 18:31:28  2015-12-21 17:34:16  2015-12-22 18:16:35   \n",
      "2  2015-12-18 17:02:23  2015-12-21 17:20:17  2015-12-22 16:32:50   \n",
      "3  2015-12-18 17:55:23  2015-12-21 16:49:09  2015-12-22 17:24:00   \n",
      "4  2015-12-18 17:52:48  2015-12-21 17:43:35  2015-12-22 18:07:57   \n",
      "\n",
      "            2015-12-23           2015-12-24 2015-12-25           2015-12-28  \\\n",
      "0  2015-12-23 16:44:44  2015-12-24 17:47:22        NaN  2015-12-28 18:00:07   \n",
      "1  2015-12-23 17:38:18                  NaN        NaN  2015-12-28 17:08:38   \n",
      "2  2015-12-23 16:59:43  2015-12-24 16:58:25        NaN  2015-12-28 16:43:31   \n",
      "3  2015-12-23 17:36:35  2015-12-24 16:48:21        NaN  2015-12-28 17:19:34   \n",
      "4  2015-12-23 18:00:49  2015-12-24 17:59:22        NaN  2015-12-28 17:44:59   \n",
      "\n",
      "            2015-12-29           2015-12-30           2015-12-31  \n",
      "0  2015-12-29 17:22:30  2015-12-30 17:40:56  2015-12-31 17:17:33  \n",
      "1  2015-12-29 17:54:46  2015-12-30 18:31:35  2015-12-31 17:40:58  \n",
      "2  2015-12-29 17:09:56  2015-12-30 17:06:25  2015-12-31 17:15:50  \n",
      "3  2015-12-29 16:58:16  2015-12-30 17:40:11  2015-12-31 17:09:14  \n",
      "4  2015-12-29 18:47:00  2015-12-30 17:15:33  2015-12-31 17:42:14  \n",
      "\n",
      "[5 rows x 262 columns] \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4410 entries, 0 to 4409\n",
      "Columns: 262 entries, Unnamed: 0 to 2015-12-31\n",
      "dtypes: float64(12), int64(1), object(249)\n",
      "memory usage: 8.8+ MB\n",
      "None \n",
      "\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "id": "56a5842c9df8e5cd"
   },
   "cell_type": "markdown",
   "source": [
    " # 4) Fusion des données"
   ],
   "id": "56a5842c9df8e5cd"
  },
  {
   "metadata": {
    "id": "dc26c6ab46b3aff5"
   },
   "cell_type": "markdown",
   "source": [
    "## 4.1) Merge general_data, employee_survey_data et manager_survey_data"
   ],
   "id": "dc26c6ab46b3aff5"
  },
  {
   "metadata": {
    "id": "53b9d56da45676f1",
    "ExecuteTime": {
     "end_time": "2025-01-30T10:44:53.798559Z",
     "start_time": "2025-01-30T10:44:53.791288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_merged = pd.merge(df_general, df_emp_survey, on=\"EmployeeID\")\n",
    "df_merged = pd.merge(df_merged, df_mgr_survey, on=\"EmployeeID\")"
   ],
   "id": "53b9d56da45676f1",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "id": "70fdc0b02f93464a"
   },
   "cell_type": "markdown",
   "source": [
    "## 4.2) Calculer le nombre d'heures travaillées par jour\n",
    "On va aussi créer des features à partir de df_in_time et df_out_time (par exemple : nombre d'heures travaillées moyennes). </br>\n",
    "Les colonnes de in_time / out_time sont des dates/heures pour chaque jour travaillé."
   ],
   "id": "70fdc0b02f93464a"
  },
  {
   "metadata": {
    "id": "947e4ad54d41597e",
    "ExecuteTime": {
     "end_time": "2025-01-30T10:46:08.865373Z",
     "start_time": "2025-01-30T10:46:01.238121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#  - Pour chaque employé, on peut calculer la différence out_time - in_time (en heures) pour chaque jour.\n",
    "#  - Ensuite on peut faire la moyenne sur toutes les dates disponibles pour obtenir \"moyenne d'heures/jour\"\n",
    "\n",
    "# Replace Unnamed header by EmployeeID\n",
    "df_in_time.rename(columns={\"Unnamed: 0\": \"EmployeeID\"}, inplace=True)\n",
    "df_out_time.rename(columns={\"Unnamed: 0\": \"EmployeeID\"}, inplace=True)\n",
    "\n",
    "# Suppression de la première colonne \"EmployeeID\" pour faciliter les opérations (on la conserve à part)\n",
    "df_in_time_id = df_in_time['EmployeeID']\n",
    "df_out_time_id = df_out_time['EmployeeID']\n",
    "\n",
    "# On exclut la colonne 'EmployeeID' des dataframes pour ne traiter que les colonnes date/heure\n",
    "df_in_time_dates = df_in_time.drop(['EmployeeID'], axis=1)\n",
    "df_out_time_dates = df_out_time.drop(['EmployeeID'], axis=1)\n",
    "\n",
    "# Conversion des valeurs string en datetime pour permettre la soustraction des temps\n",
    "# Nota : Certaines valeurs sont \"NA\" => conversion en NaT\n",
    "df_in_time_dates = df_in_time_dates.apply(pd.to_datetime, errors='coerce')\n",
    "df_out_time_dates = df_out_time_dates.apply(pd.to_datetime, errors='coerce')\n",
    "\n",
    "# Calcul de la différence (out_time - in_time) par employé et par jour\n",
    "df_hours = df_out_time_dates - df_in_time_dates  # Résultat en format timedelta\n",
    "\n",
    "# Convertir les timedelta en nombre d'heures (float)\n",
    "df_hours = df_hours.apply(lambda x: x.dt.total_seconds() / 3600)\n",
    "\n",
    "# Exemple : Calcul d'une statistique agrégée (moyenne d'heures/jour travaillé) pour chaque employé\n",
    "df_hours_mean = df_hours.mean(axis=1)\n",
    "\n",
    "# On veut garder la différence entre StandardHours et mean_work_hours\n",
    "df_hours['mean_work_hours_diff'] = df_hours_mean - df_merged['StandardHours']\n",
    "\n",
    "# On peut aussi calculer le nombre de jours d'absence (journées entières manquantes => in_time = NA & out_time = NA)\n",
    "# ou le ratio de jours travaillés vs le total possible, etc.\n",
    "# Ci-dessous un exemple de calcul du nombre de jours (colonnes) pour lesquels l'entrée est manquante\n",
    "nb_jours_total = df_hours.shape[1] - 1  # -1 car la dernière colonne est 'mean_work_hours' qu'on vient d'ajouter\n",
    "df_hours['absent_days'] = df_hours.iloc[:, :-1].isna().sum(axis=1)  # On ne compte pas la col. 'mean_work_hours'\n",
    "\n",
    "# Concaténer EmployeeID pour pouvoir refusionner\n",
    "df_hours_final = pd.concat([df_in_time_id, df_hours[['mean_work_hours_diff','absent_days']]], axis=1)\n",
    "\n",
    "# Heure d'arrivé moyenne\n",
    "df_hours_final['Avg_Hours_In'] = df_in_time_dates.apply(\n",
    "    lambda row: row.dropna().dt.hour.mean() + row.dropna().dt.minute.mean() / 60\n",
    "    if not row.dropna().empty else None, axis=1\n",
    ")\n",
    "\n",
    "# Heure de départ moyenne\n",
    "df_hours_final['Avg_Hours_Out'] = df_out_time_dates.apply(\n",
    "    lambda row: row.dropna().dt.hour.mean() + row.dropna().dt.minute.mean() / 60\n",
    "    if not row.dropna().empty else None, axis=1\n",
    ")"
   ],
   "id": "947e4ad54d41597e",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "id": "dd78ce3782756295"
   },
   "cell_type": "markdown",
   "source": [
    "## 4.3) Merge avec df_merged pour rajouter ces nouvelles features"
   ],
   "id": "dd78ce3782756295"
  },
  {
   "metadata": {
    "id": "1c4b1a2d849d0160",
    "ExecuteTime": {
     "end_time": "2025-01-30T10:44:45.976261Z",
     "start_time": "2025-01-30T10:44:45.960813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_merged = pd.merge(df_merged, df_hours_final, on='EmployeeID', how='left')\n",
    "\n",
    "print(\"\\n=== Aperçu des données fusionnées ===\\n\")\n",
    "print(df_merged.head())\n",
    "print(df_merged.info())"
   ],
   "id": "1c4b1a2d849d0160",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Aperçu des données fusionnées ===\n",
      "\n",
      "   Age Attrition     BusinessTravel              Department  DistanceFromHome  \\\n",
      "0   51        No      Travel_Rarely                   Sales                 6   \n",
      "1   31       Yes  Travel_Frequently  Research & Development                10   \n",
      "2   32        No  Travel_Frequently  Research & Development                17   \n",
      "3   38        No         Non-Travel  Research & Development                 2   \n",
      "4   32        No      Travel_Rarely  Research & Development                10   \n",
      "\n",
      "   Education EducationField  EmployeeCount  EmployeeID  Gender  ...  \\\n",
      "0          2  Life Sciences              1           1  Female  ...   \n",
      "1          1  Life Sciences              1           2  Female  ...   \n",
      "2          4          Other              1           3    Male  ...   \n",
      "3          5  Life Sciences              1           4    Male  ...   \n",
      "4          1        Medical              1           5    Male  ...   \n",
      "\n",
      "   YearsWithCurrManager EnvironmentSatisfaction JobSatisfaction  \\\n",
      "0                     0                     3.0             4.0   \n",
      "1                     4                     3.0             2.0   \n",
      "2                     3                     2.0             2.0   \n",
      "3                     5                     4.0             4.0   \n",
      "4                     4                     4.0             1.0   \n",
      "\n",
      "   WorkLifeBalance  JobInvolvement PerformanceRating  mean_work_hours_diff  \\\n",
      "0              2.0               3                 3             -0.626349   \n",
      "1              4.0               2                 4             -0.281031   \n",
      "2              1.0               3                 3             -0.986760   \n",
      "3              3.0               2                 3             -0.806322   \n",
      "4              3.0               3                 3              0.006175   \n",
      "\n",
      "   absent_days  Avg_Hours_In  Avg_Hours_Out  \n",
      "0           29      9.993032      17.367170  \n",
      "1           25      9.980720      17.698446  \n",
      "2           19     10.016598      17.030096  \n",
      "3           26      9.973830      17.167305  \n",
      "4           16      9.990068      17.996327  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4410 entries, 0 to 4409\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Age                      4410 non-null   int64  \n",
      " 1   Attrition                4410 non-null   object \n",
      " 2   BusinessTravel           4410 non-null   object \n",
      " 3   Department               4410 non-null   object \n",
      " 4   DistanceFromHome         4410 non-null   int64  \n",
      " 5   Education                4410 non-null   int64  \n",
      " 6   EducationField           4410 non-null   object \n",
      " 7   EmployeeCount            4410 non-null   int64  \n",
      " 8   EmployeeID               4410 non-null   int64  \n",
      " 9   Gender                   4410 non-null   object \n",
      " 10  JobLevel                 4410 non-null   int64  \n",
      " 11  JobRole                  4410 non-null   object \n",
      " 12  MaritalStatus            4410 non-null   object \n",
      " 13  MonthlyIncome            4410 non-null   int64  \n",
      " 14  NumCompaniesWorked       4391 non-null   float64\n",
      " 15  Over18                   4410 non-null   object \n",
      " 16  PercentSalaryHike        4410 non-null   int64  \n",
      " 17  StandardHours            4410 non-null   int64  \n",
      " 18  StockOptionLevel         4410 non-null   int64  \n",
      " 19  TotalWorkingYears        4401 non-null   float64\n",
      " 20  TrainingTimesLastYear    4410 non-null   int64  \n",
      " 21  YearsAtCompany           4410 non-null   int64  \n",
      " 22  YearsSinceLastPromotion  4410 non-null   int64  \n",
      " 23  YearsWithCurrManager     4410 non-null   int64  \n",
      " 24  EnvironmentSatisfaction  4385 non-null   float64\n",
      " 25  JobSatisfaction          4390 non-null   float64\n",
      " 26  WorkLifeBalance          4372 non-null   float64\n",
      " 27  JobInvolvement           4410 non-null   int64  \n",
      " 28  PerformanceRating        4410 non-null   int64  \n",
      " 29  mean_work_hours_diff     4410 non-null   float64\n",
      " 30  absent_days              4410 non-null   int64  \n",
      " 31  Avg_Hours_In             4410 non-null   float64\n",
      " 32  Avg_Hours_Out            4410 non-null   float64\n",
      "dtypes: float64(8), int64(17), object(8)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyse de la distribution normale des données"
   ],
   "metadata": {
    "id": "NV03Tx6HQnIX"
   },
   "id": "NV03Tx6HQnIX"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "\n",
    "def plot_normal_distribution(df, column_name, log_transform=False, log_base='10'):\n",
    "    \"\"\"\n",
    "    Trace un histogramme (densité) de la colonne `column_name` d'un DataFrame `df`\n",
    "    et superpose la courbe de la distribution normale basée sur la moyenne et\n",
    "    l'écart-type (sur données brutes ou log-transformées).\n",
    "\n",
    "    Paramètres:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Votre DataFrame\n",
    "    column_name : str\n",
    "        Nom de la colonne à tracer\n",
    "    log_transform : bool\n",
    "        Si True, applique un log sur les valeurs de la colonne avant le tracé.\n",
    "    log_base : str\n",
    "        - '10' pour un log base 10 (log10)\n",
    "        - 'e' pour un log naturel (ln)\n",
    "    \"\"\"\n",
    "    data = df[column_name].dropna()\n",
    "\n",
    "    if log_transform:\n",
    "        if log_base == '10':\n",
    "            data = np.log10(data + 1)\n",
    "            suffix = \" (log10)\"\n",
    "        elif log_base == 'e':\n",
    "            data = np.log(data + 1)\n",
    "            suffix = \" (ln)\"\n",
    "        else:\n",
    "            raise ValueError(\"log_base doit être '10' ou 'e'\")\n",
    "    else:\n",
    "        suffix = \"\"\n",
    "\n",
    "\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "\n",
    "    x_min = mean - 3 * std\n",
    "    x_max = mean + 3 * std\n",
    "\n",
    "    if x_min == x_max:\n",
    "        x_min = mean - 1\n",
    "        x_max = mean + 1\n",
    "\n",
    "    x = np.linspace(x_min, x_max, 200)\n",
    "\n",
    "    pdf = norm.pdf(x, mean, std)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(data, kde=False, stat='density', bins=30, color='skyblue', label='Données')\n",
    "\n",
    "    plt.plot(x, pdf, color='red', linewidth=2, label='Distribution normale')\n",
    "    plt.xlim(x_min, x_max)\n",
    "\n",
    "    plt.title(f\"Distribution de la colonne: {column_name}{suffix}\")\n",
    "    plt.xlabel(f\"{column_name}{suffix}\")\n",
    "    plt.ylabel(\"Densité\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "id": "Y8gVwKT_QsiY"
   },
   "id": "Y8gVwKT_QsiY",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_normal_distribution(df_merged, 'MonthlyIncome', log_transform=False)\n",
    "plot_normal_distribution(df_merged, 'MonthlyIncome', log_transform=True, log_base='10')\n",
    "plot_normal_distribution(df_merged, 'mean_work_hours_diff', log_transform=False, log_base='10')"
   ],
   "metadata": {
    "id": "fWQdZYsqQ09z"
   },
   "id": "fWQdZYsqQ09z",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "2820c7d5605eed20"
   },
   "cell_type": "markdown",
   "source": [
    "# 5) Nettoyage et préparation des données"
   ],
   "id": "2820c7d5605eed20"
  },
  {
   "metadata": {
    "id": "5a9b6e778e5377c0"
   },
   "cell_type": "markdown",
   "source": [
    "## 5.1) Gérer les valeurs manquantes"
   ],
   "id": "5a9b6e778e5377c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:38:47.174692Z",
     "start_time": "2025-01-27T08:38:47.169880Z"
    },
    "id": "7bb686111f61c3ce"
   },
   "cell_type": "code",
   "source": [
    "# On regarde déjà combien de valeurs manquantes par colonne\n",
    "missing_values = df_merged.isnull().sum()\n",
    "print(\"\\n=== Nombre de valeurs manquantes par colonne ===\\n\", missing_values)"
   ],
   "id": "7bb686111f61c3ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:38:47.224229Z",
     "start_time": "2025-01-27T08:38:47.220647Z"
    },
    "id": "36fd2752b3b6e64b"
   },
   "cell_type": "code",
   "source": [
    "# Dropna sur la colonne Attrition car c'est notre target\n",
    "df_merged = df_merged.dropna(subset=['Attrition'])"
   ],
   "id": "36fd2752b3b6e64b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:38:47.279926Z",
     "start_time": "2025-01-27T08:38:47.275526Z"
    },
    "id": "ca2427042f08cf3c"
   },
   "cell_type": "code",
   "source": [
    "# Exemple d'imputation : pour 'mean_work_hours' et 'absent_days', on remplace les NaN par la moyenne\n",
    "# On privilégie la moyenne ici car les valeurs de type durée (heures) ne sont pas fortement bornées\n",
    "df_merged['mean_work_hours_diff'] = df_merged['mean_work_hours_diff'].fillna(df_merged['mean_work_hours_diff'].mean())\n",
    "print(f\"Filled missing values for column 'mean_work_hours_diff' with mean value {df_merged['mean_work_hours_diff'].mean()}\")\n",
    "df_merged['absent_days'] = df_merged['absent_days'].fillna(df_merged['absent_days'].mean())\n",
    "print(f\"Filled missing values for column 'absent_days' with mean value {df_merged['absent_days'].mean()}\")"
   ],
   "id": "ca2427042f08cf3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:38:47.332710Z",
     "start_time": "2025-01-27T08:38:47.326594Z"
    },
    "id": "3bcb7a7cbf3faa1e"
   },
   "cell_type": "code",
   "source": [
    "# Pour les colonnes de satisfaction ou autres colonnes numériques manquantes, on peut aussi faire un fillna\n",
    "# On privilégie la médiane\n",
    "num_cols = ['EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance',\n",
    "            'JobInvolvement', 'PerformanceRating', 'TotalWorkingYears', 'NumCompaniesWorked',]\n",
    "for col in num_cols:\n",
    "    if col in df_merged.columns:\n",
    "        df_merged[col] = df_merged[col].fillna(df_merged[col].median())\n",
    "        print(f\"Filled missing values for column {col} with median value {df_merged[col].median()}\")\n"
   ],
   "id": "3bcb7a7cbf3faa1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "f1cf019fe192eb53"
   },
   "cell_type": "markdown",
   "source": [
    "## 5.2) Nettoyage de certaines colonnes (ex: Over18, EmployeeCount, StandardHours)"
   ],
   "id": "f1cf019fe192eb53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:38:47.385065Z",
     "start_time": "2025-01-27T08:38:47.379072Z"
    },
    "id": "cd8762219021a0a5"
   },
   "cell_type": "code",
   "source": [
    "# On remarque souvent dans ce dataset \"EmployeeCount\" est toujours 1 => pas d'intérêt\n",
    "# \"Over18\" est toujours \"Y\", \"StandardHours\" est souvent 8 => on peut les drop\n",
    "cols_to_drop = ['Over18','StandardHours','EmployeeCount']\n",
    "for c in cols_to_drop:\n",
    "    if c in df_merged.columns:\n",
    "        df_merged.drop(c, axis=1, inplace=True, errors='ignore')"
   ],
   "id": "cd8762219021a0a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "1c95da9bd6241297"
   },
   "cell_type": "markdown",
   "source": [
    "## 5.3) Conversion de colonnes catégorielles en numériques"
   ],
   "id": "1c95da9bd6241297"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:38:47.442663Z",
     "start_time": "2025-01-27T08:38:47.434602Z"
    },
    "id": "d947248ca8f41304"
   },
   "cell_type": "code",
   "source": [
    "# Par exemple, Attrition = Yes/No, Gender = Male/Female, etc.\n",
    "# On peut les encoder, soit via LabelEncoder, soit via OneHotEncoder\n",
    "# Commençons par un label encoding simple pour la variable cible\n",
    "\n",
    "df_merged['Attrition'] = df_merged['Attrition'].map({'Yes':1, 'No':0})\n",
    "\n",
    "# Autres colonnes catégorielles (BusinessTravel, Department, EducationField, Gender, MaritalStatus, JobRole...)\n",
    "cat_cols = ['BusinessTravel','Department','EducationField','Gender','MaritalStatus','JobRole']\n",
    "\n",
    "# On va faire un one-hot-encoding rapide:\n",
    "df_merged = pd.get_dummies(df_merged, columns=cat_cols, drop_first=True)"
   ],
   "id": "d947248ca8f41304",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "15e665b9971bbec1"
   },
   "cell_type": "markdown",
   "source": [
    "# 6) Séparation des données en train/test"
   ],
   "id": "15e665b9971bbec1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:38:47.489216Z",
     "start_time": "2025-01-27T08:38:47.481745Z"
    },
    "id": "b75e224a476a1de4"
   },
   "cell_type": "code",
   "source": [
    "# On sépare la cible (Attrition) des features\n",
    "\n",
    "X = df_merged.drop(['EmployeeID','Attrition'], axis=1)\n",
    "y = df_merged['Attrition']\n",
    "\n",
    "# Ensuite on fait un split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")"
   ],
   "id": "b75e224a476a1de4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "ba440a38f1d808be"
   },
   "cell_type": "markdown",
   "source": [
    "# 7) scaling et entraînement d'un modèle simple"
   ],
   "id": "ba440a38f1d808be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:38:47.562265Z",
     "start_time": "2025-01-27T08:38:47.552581Z"
    },
    "id": "4f5d93c6722c17d0"
   },
   "cell_type": "code",
   "source": [
    "# Selon le modèle (Logistic Regression par exemple), il peut être intéressant de standardiser\n",
    "# Ici on va montrer un exemple de pipeline manuel (scaling + logistic regression).\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "id": "4f5d93c6722c17d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "b24bf4ba78ad12e7"
   },
   "cell_type": "markdown",
   "source": [
    "## 7.1) Entrainement d'un modèle de Logistic Regression"
   ],
   "id": "b24bf4ba78ad12e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:38:47.619789Z",
     "start_time": "2025-01-27T08:38:47.606490Z"
    },
    "id": "b4402e4358f0af3f"
   },
   "cell_type": "code",
   "source": [
    "clf_lr = LogisticRegression(random_state=42, max_iter=500)\n",
    "clf_lr.fit(X_train_scaled, y_train)"
   ],
   "id": "b4402e4358f0af3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "14bafbac95b52eb0"
   },
   "cell_type": "markdown",
   "source": [
    "## 7.2) Prediction et évaluation"
   ],
   "id": "14bafbac95b52eb0"
  },
  {
   "metadata": {
    "id": "c3fdcdc214d274b3"
   },
   "cell_type": "markdown",
   "source": [
    "### 7.2.1) Prediction"
   ],
   "id": "c3fdcdc214d274b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:38:47.731833Z",
     "start_time": "2025-01-27T08:38:47.728760Z"
    },
    "id": "5ae717bee20a0cab"
   },
   "cell_type": "code",
   "source": [
    "y_pred_lr = clf_lr.predict(X_test_scaled)\n",
    "y_proba_lr = clf_lr.predict_proba(X_test_scaled)[:,1]"
   ],
   "id": "5ae717bee20a0cab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "5bcb83ca218971a8"
   },
   "cell_type": "markdown",
   "source": [
    "### 7.2.2) Evaluation"
   ],
   "id": "5bcb83ca218971a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:38:47.799284Z",
     "start_time": "2025-01-27T08:38:47.787627Z"
    },
    "id": "9605cd8f008e1afd"
   },
   "cell_type": "code",
   "source": [
    "# Évaluation\n",
    "print(\"=== Évaluation Logistic Regression ===\")\n",
    "print(\"Accuracy : \", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"ROC AUC  : \", roc_auc_score(y_test, y_proba_lr))\n",
    "print(\"Matrice de confusion :\\n\", confusion_matrix(y_test, y_pred_lr))\n",
    "print(\"Classification report :\\n\", classification_report(y_test, y_pred_lr))"
   ],
   "id": "9605cd8f008e1afd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "ac0b7aa33f9db48"
   },
   "cell_type": "markdown",
   "source": [
    "## 7.3) Entrainement d'un modèle de Random Forest"
   ],
   "id": "ac0b7aa33f9db48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T16:18:42.829550Z",
     "start_time": "2025-01-28T16:18:42.186938Z"
    },
    "id": "ce2a502433dfc066"
   },
   "cell_type": "code",
   "source": [
    "clf_rf = RandomForestClassifier(random_state=42)\n",
    "clf_rf.fit(X_train, y_train)  # On peut le tester sans scaling\n",
    "y_pred_rf = clf_rf.predict(X_test)\n",
    "y_proba_rf = clf_rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"\\n=== Évaluation Random Forest ===\")\n",
    "print(\"Accuracy : \", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"ROC AUC  : \", roc_auc_score(y_test, y_proba_rf))\n",
    "print(\"Legende de la matrice de confusion :\")\n",
    "print(\"TN FP\")\n",
    "print(\"FN TP\")\n",
    "print(\"Matrice de confusion :\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"Classification report :\\n\", classification_report(y_test, y_pred_rf))"
   ],
   "id": "ce2a502433dfc066",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "f29264be8969a2bc"
   },
   "cell_type": "markdown",
   "source": [
    "# 8) Optimisation des hyperparamètres"
   ],
   "id": "f29264be8969a2bc"
  },
  {
   "metadata": {
    "id": "cc5f76fd9f790aa4"
   },
   "cell_type": "markdown",
   "source": [
    "## 8.1) Tuning de la Logistic Regression"
   ],
   "id": "cc5f76fd9f790aa4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:38:58.544505Z",
     "start_time": "2025-01-27T08:38:55.833895Z"
    },
    "id": "350a66b0af807a"
   },
   "cell_type": "code",
   "source": [
    "# Exemple de grille de paramètres (simple).\n",
    "# Attention : certaines combinaisons penalty/solver peuvent être incompatibles.\n",
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],         # Coefficient de régularisation\n",
    "    'solver': ['liblinear', 'lbfgs'],     # Solveur\n",
    "    'max_iter': [100, 200, 500]           # Nombre itérations max\n",
    "    # 'penalty': ['l1','l2']  # <--- l1 nécessite solver='liblinear';\n",
    "                              #      si on veut tester l1, il faut ajuster la grille\n",
    "}\n",
    "\n",
    "# Création de l'instance LogisticRegression\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# GridSearchCV : 5-fold cross-validation, scoring basé sur l'accuracy (ou 'roc_auc', etc.)\n",
    "grid_lr = GridSearchCV(\n",
    "    estimator=lr_model,\n",
    "    param_grid=param_grid_lr,\n",
    "    scoring='accuracy',        # ou 'roc_auc'\n",
    "    cv=5,                      # 5 folds\n",
    "    verbose=1,                 # pour voir la progression\n",
    "    n_jobs=-1                  # utilise tous les coeurs dispo\n",
    ")\n",
    "\n",
    "# Entraînement du GridSearchCV\n",
    "grid_lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Récupération des meilleurs paramètres et score\n",
    "print(\"=== Logistic Regression - Meilleurs hyperparamètres ===\")\n",
    "print(\"Best Params :\", grid_lr.best_params_)\n",
    "print(\"Best Score  :\", grid_lr.best_score_)\n",
    "\n",
    "# On peut maintenant re-prédire sur le test set avec le meilleur modèle trouvé\n",
    "best_lr = grid_lr.best_estimator_\n",
    "y_pred_best_lr = best_lr.predict(X_test_scaled)\n",
    "y_proba_best_lr = best_lr.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "print(\"\\n=== Évaluation sur le jeu de test (LogisticRegression avec meilleurs params) ===\")\n",
    "print(\"Accuracy :\", accuracy_score(y_test, y_pred_best_lr))\n",
    "print(\"ROC AUC  :\", roc_auc_score(y_test, y_proba_best_lr))\n",
    "print(\"Matrice de confusion :\\n\", confusion_matrix(y_test, y_pred_best_lr))\n",
    "print(\"Classification report :\\n\", classification_report(y_test, y_pred_best_lr))"
   ],
   "id": "350a66b0af807a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "b6600df29ea5de43"
   },
   "cell_type": "markdown",
   "source": [
    "### 8.1.1) Visualisation des résultats"
   ],
   "id": "b6600df29ea5de43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:40:18.661599Z",
     "start_time": "2025-01-27T08:40:18.550418Z"
    },
    "id": "3175fdd6dbedfaf2"
   },
   "cell_type": "code",
   "source": [
    "# Exemple : on peut faire un DataFrame à partir de grid_lr.cv_results_ pour tracer un heatmap\n",
    "# entre C et max_iter, en séparant par solver.\n",
    "# Cela nécessite un pivot de la table.\n",
    "\n",
    "results_lr = pd.DataFrame(grid_lr.cv_results_)\n",
    "\n",
    "# Pour simplifier, on ne visualise que le solver='lbfgs', par exemple\n",
    "df_lbfgs = results_lr[ results_lr['param_solver'] == 'lbfgs' ]\n",
    "# Pivot => index: param_C, columns: param_max_iter, values: mean_test_score\n",
    "pivot_lbfgs = df_lbfgs.pivot(\n",
    "    index='param_C',\n",
    "    columns='param_max_iter',\n",
    "    values='mean_test_score'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(pivot_lbfgs, annot=True, cmap='viridis')\n",
    "plt.title(\"GridSearchCV - LogisticRegression (solver='lbfgs')\")\n",
    "plt.ylabel(\"C\")\n",
    "plt.xlabel(\"max_iter\")\n",
    "plt.show()"
   ],
   "id": "3175fdd6dbedfaf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "209ce321f3dc5e31"
   },
   "cell_type": "markdown",
   "source": [
    "## 8.2) Tuning du Random Forest"
   ],
   "id": "209ce321f3dc5e31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:41:28.987041Z",
     "start_time": "2025-01-27T08:41:16.124717Z"
    },
    "id": "e03c4276f189d994"
   },
   "cell_type": "code",
   "source": [
    "# Exemple de grille de paramètres\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "    # on peut rajouter 'max_features', 'min_samples_leaf', etc. selon les besoins\n",
    "}\n",
    "\n",
    "# l'objectif du random_state est de garantir la reproductibilité des résultats. La valeur 42 est arbitraire.\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_rf = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid_rf,\n",
    "    scoring='accuracy',  # ou 'roc_auc'\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_rf.fit(X_train, y_train)\n",
    "print(\"\\n=== Random Forest - Meilleurs hyperparamètres ===\")\n",
    "print(\"Best Params :\", grid_rf.best_params_)\n",
    "print(\"Best Score  :\", grid_rf.best_score_)\n",
    "\n",
    "best_rf = grid_rf.best_estimator_\n",
    "y_pred_best_rf = best_rf.predict(X_test)\n",
    "y_proba_best_rf = best_rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"\\n=== Évaluation sur le jeu de test (RandomForest avec meilleurs params) ===\")\n",
    "print(\"Accuracy :\", accuracy_score(y_test, y_pred_best_rf))\n",
    "print(\"ROC AUC  :\", roc_auc_score(y_test, y_proba_best_rf))\n",
    "print(\"Matrice de confusion :\\n\", confusion_matrix(y_test, y_pred_best_rf))\n",
    "print(\"Classification report :\\n\", classification_report(y_test, y_pred_best_rf))"
   ],
   "id": "e03c4276f189d994",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "58f2ea304693be56"
   },
   "cell_type": "markdown",
   "source": [
    "### 8.2.1) Visualisation des résultats"
   ],
   "id": "58f2ea304693be56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:42:02.600812Z",
     "start_time": "2025-01-27T08:42:02.498032Z"
    },
    "id": "908a289f87b914e2"
   },
   "cell_type": "code",
   "source": [
    "results_rf = pd.DataFrame(grid_rf.cv_results_)\n",
    "\n",
    "# Exemple d'extraction de la partie n_estimators / max_depth (pivot sur 2 variables)\n",
    "# On fixe min_samples_split=2 par exemple pour la visualisation\n",
    "df_split2 = results_rf[ results_rf['param_min_samples_split'] == 2 ]\n",
    "\n",
    "pivot_rf = df_split2.pivot(\n",
    "    index='param_n_estimators',\n",
    "    columns='param_max_depth',\n",
    "    values='mean_test_score'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(pivot_rf, annot=True, cmap='coolwarm')\n",
    "plt.title(\"GridSearchCV - RandomForest (min_samples_split=2)\")\n",
    "plt.ylabel(\"n_estimators\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.show()"
   ],
   "id": "908a289f87b914e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "cfd0b6a5902c1f8"
   },
   "cell_type": "markdown",
   "source": [
    "# 9) Feature importance"
   ],
   "id": "cfd0b6a5902c1f8"
  },
  {
   "metadata": {
    "id": "a165d53c00bafbf7"
   },
   "cell_type": "markdown",
   "source": [
    "## 8.1) Random Forest"
   ],
   "id": "a165d53c00bafbf7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:42:05.908544Z",
     "start_time": "2025-01-27T08:42:05.818567Z"
    },
    "id": "cd952ef566ac7f71"
   },
   "cell_type": "code",
   "source": [
    "# Par exemple, on peut rapidement regarder l'importance des features dans le Random Forest :\n",
    "importances = clf_rf.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "feat_importances = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "print(\"\\n=== Feature importances (RandomForest) ===\\n\", feat_importances.head(10))\n",
    "\n",
    "# On peut éventuellement tracer un barplot pour visualiser\n",
    "plt.figure(figsize=(8,6))\n",
    "feat_importances.head(10).plot(kind='bar')\n",
    "plt.title(\"Top 10 des features les plus importantes (RandomForest)\")\n",
    "plt.show()\n"
   ],
   "id": "cd952ef566ac7f71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "8e86c9d09b9a08b0"
   },
   "cell_type": "markdown",
   "source": [
    "## 8.2) Logistic Regression"
   ],
   "id": "8e86c9d09b9a08b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:32:37.498898Z",
     "start_time": "2025-01-27T08:32:37.405125Z"
    },
    "id": "358304d63dc89c7d"
   },
   "cell_type": "code",
   "source": [
    "# Pour la régression logistique, on peut regarder les coefficients associés à chaque feature\n",
    "# On peut aussi regarder les coefficients les plus importants\n",
    "coefs = clf_lr.coef_[0]\n",
    "feat_coefs = pd.Series(coefs, index=feature_names).sort_values(ascending=False)\n",
    "print(\"\\n=== Coefficients (Logistic Regression) ===\\n\", feat_coefs.head(10))\n",
    "\n",
    "# On peut aussi tracer un barplot pour visualiser\n",
    "plt.figure(figsize=(8,6))\n",
    "feat_coefs.head(10).plot(kind='bar')\n",
    "plt.title(\"Top 10 des coefficients les plus importants (Logistic Regression)\")\n",
    "plt.show()"
   ],
   "id": "358304d63dc89c7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "59554f52c7cbad24"
   },
   "cell_type": "markdown",
   "source": [
    "# 10) Evaluation finale"
   ],
   "id": "59554f52c7cbad24"
  },
  {
   "metadata": {
    "id": "d3c7d0d4d0f22e30"
   },
   "cell_type": "markdown",
   "source": [
    "## 10.1) Train et test scores"
   ],
   "id": "d3c7d0d4d0f22e30"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T12:42:41.932755Z",
     "start_time": "2025-01-27T12:42:41.841559Z"
    },
    "id": "7060a4c14a4a7873"
   },
   "cell_type": "code",
   "source": [
    "# On peut regarder les scores finaux sur le jeu de train et de test\n",
    "print(\"\\n=== Scores finaux ===\")\n",
    "print(\"Accuracy (Train) : \", accuracy_score(y_train, best_rf.predict(X_train)))\n",
    "print(\"Accuracy (Test)  : \", accuracy_score(y_test, y_pred_best_rf))\n",
    "print(\"ROC AUC  (Train) : \", roc_auc_score(y_train, best_rf.predict_proba(X_train)[:,1]))\n",
    "print(\"ROC AUC  (Test)  : \", roc_auc_score(y_test, y_proba_best_rf))"
   ],
   "id": "7060a4c14a4a7873",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "584090798eee38c3"
   },
   "cell_type": "markdown",
   "source": [
    "## 10.2) Courbe ROC"
   ],
   "id": "584090798eee38c3"
  },
  {
   "metadata": {
    "id": "ede554a708d6898f"
   },
   "cell_type": "markdown",
   "source": [
    "Nous pouvons constater que la courbe ROC a de bonnes performances pour le modèle Random Forest."
   ],
   "id": "ede554a708d6898f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T13:14:53.664333Z",
     "start_time": "2025-01-27T13:14:53.582732Z"
    },
    "id": "56e7b723c73ac2c1"
   },
   "cell_type": "code",
   "source": [
    "# On peut aussi tracer la courbe ROC\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_proba_best_rf)\n",
    "fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, y_proba_best_lr)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr_rf, tpr_rf, label='RandomForest', c='b')\n",
    "plt.plot(fpr_lr, tpr_lr, label='LogisticRegression', c='r')\n",
    "plt.plot([0,1], [0,1], '--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Courbe ROC')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "56e7b723c73ac2c1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
