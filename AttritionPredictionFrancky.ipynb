{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1) Importation des librairies",
   "id": "124bbaf532a0cd66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:10:35.434153Z",
     "start_time": "2025-01-27T08:10:35.431367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pour la séparation des données en train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pour la création et l'évaluation de modèles\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score"
   ],
   "id": "45d03adf497a0cdf",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2) Chargement des données",
   "id": "59d618d81bb602ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:10:36.166615Z",
     "start_time": "2025-01-27T08:10:35.441617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Lis les différents fichiers CSV\n",
    "folder = \"src/\"\n",
    "df_general = pd.read_csv(folder + \"general_data.csv\")\n",
    "df_emp_survey = pd.read_csv(folder + \"employee_survey_data.csv\")\n",
    "df_mgr_survey = pd.read_csv(folder + \"manager_survey_data.csv\")\n",
    "df_in_time = pd.read_csv(folder + \"in_time.csv\")\n",
    "df_out_time = pd.read_csv(folder + \"out_time.csv\")"
   ],
   "id": "71cf01ea2425e2b1",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3) Exploration des données",
   "id": "7b1dc6ecee341936"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:10:36.244528Z",
     "start_time": "2025-01-27T08:10:36.209700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=== General Data ===\")\n",
    "print(df_general.head(), \"\\n\")\n",
    "print(df_general.info(), \"\\n\")\n",
    "\n",
    "print(\"=== Employee Survey Data ===\")\n",
    "print(df_emp_survey.head(), \"\\n\")\n",
    "print(df_emp_survey.info(), \"\\n\")\n",
    "\n",
    "print(\"=== Manager Survey Data ===\")\n",
    "print(df_mgr_survey.head(), \"\\n\")\n",
    "print(df_mgr_survey.info(), \"\\n\")\n",
    "\n",
    "print(\"=== In Time Data ===\")\n",
    "print(df_in_time.head(), \"\\n\")\n",
    "print(df_in_time.info(), \"\\n\")\n",
    "\n",
    "print(\"=== Out Time Data ===\")\n",
    "print(df_out_time.head(), \"\\n\")\n",
    "print(df_out_time.info(), \"\\n\")"
   ],
   "id": "7ea3ad16748652c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== General Data ===\n",
      "   Age Attrition     BusinessTravel              Department  DistanceFromHome  \\\n",
      "0   51        No      Travel_Rarely                   Sales                 6   \n",
      "1   31       Yes  Travel_Frequently  Research & Development                10   \n",
      "2   32        No  Travel_Frequently  Research & Development                17   \n",
      "3   38        No         Non-Travel  Research & Development                 2   \n",
      "4   32        No      Travel_Rarely  Research & Development                10   \n",
      "\n",
      "   Education EducationField  EmployeeCount  EmployeeID  Gender  ...  \\\n",
      "0          2  Life Sciences              1           1  Female  ...   \n",
      "1          1  Life Sciences              1           2  Female  ...   \n",
      "2          4          Other              1           3    Male  ...   \n",
      "3          5  Life Sciences              1           4    Male  ...   \n",
      "4          1        Medical              1           5    Male  ...   \n",
      "\n",
      "   NumCompaniesWorked Over18 PercentSalaryHike  StandardHours  \\\n",
      "0                 1.0      Y                11              8   \n",
      "1                 0.0      Y                23              8   \n",
      "2                 1.0      Y                15              8   \n",
      "3                 3.0      Y                11              8   \n",
      "4                 4.0      Y                12              8   \n",
      "\n",
      "   StockOptionLevel TotalWorkingYears  TrainingTimesLastYear  YearsAtCompany  \\\n",
      "0                 0               1.0                      6               1   \n",
      "1                 1               6.0                      3               5   \n",
      "2                 3               5.0                      2               5   \n",
      "3                 3              13.0                      5               8   \n",
      "4                 2               9.0                      2               6   \n",
      "\n",
      "   YearsSinceLastPromotion  YearsWithCurrManager  \n",
      "0                        0                     0  \n",
      "1                        1                     4  \n",
      "2                        0                     3  \n",
      "3                        7                     5  \n",
      "4                        0                     4  \n",
      "\n",
      "[5 rows x 24 columns] \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4410 entries, 0 to 4409\n",
      "Data columns (total 24 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Age                      4410 non-null   int64  \n",
      " 1   Attrition                4410 non-null   object \n",
      " 2   BusinessTravel           4410 non-null   object \n",
      " 3   Department               4410 non-null   object \n",
      " 4   DistanceFromHome         4410 non-null   int64  \n",
      " 5   Education                4410 non-null   int64  \n",
      " 6   EducationField           4410 non-null   object \n",
      " 7   EmployeeCount            4410 non-null   int64  \n",
      " 8   EmployeeID               4410 non-null   int64  \n",
      " 9   Gender                   4410 non-null   object \n",
      " 10  JobLevel                 4410 non-null   int64  \n",
      " 11  JobRole                  4410 non-null   object \n",
      " 12  MaritalStatus            4410 non-null   object \n",
      " 13  MonthlyIncome            4410 non-null   int64  \n",
      " 14  NumCompaniesWorked       4391 non-null   float64\n",
      " 15  Over18                   4410 non-null   object \n",
      " 16  PercentSalaryHike        4410 non-null   int64  \n",
      " 17  StandardHours            4410 non-null   int64  \n",
      " 18  StockOptionLevel         4410 non-null   int64  \n",
      " 19  TotalWorkingYears        4401 non-null   float64\n",
      " 20  TrainingTimesLastYear    4410 non-null   int64  \n",
      " 21  YearsAtCompany           4410 non-null   int64  \n",
      " 22  YearsSinceLastPromotion  4410 non-null   int64  \n",
      " 23  YearsWithCurrManager     4410 non-null   int64  \n",
      "dtypes: float64(2), int64(14), object(8)\n",
      "memory usage: 827.0+ KB\n",
      "None \n",
      "\n",
      "=== Employee Survey Data ===\n",
      "   EmployeeID  EnvironmentSatisfaction  JobSatisfaction  WorkLifeBalance\n",
      "0           1                      3.0              4.0              2.0\n",
      "1           2                      3.0              2.0              4.0\n",
      "2           3                      2.0              2.0              1.0\n",
      "3           4                      4.0              4.0              3.0\n",
      "4           5                      4.0              1.0              3.0 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4410 entries, 0 to 4409\n",
      "Data columns (total 4 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   EmployeeID               4410 non-null   int64  \n",
      " 1   EnvironmentSatisfaction  4385 non-null   float64\n",
      " 2   JobSatisfaction          4390 non-null   float64\n",
      " 3   WorkLifeBalance          4372 non-null   float64\n",
      "dtypes: float64(3), int64(1)\n",
      "memory usage: 137.9 KB\n",
      "None \n",
      "\n",
      "=== Manager Survey Data ===\n",
      "   EmployeeID  JobInvolvement  PerformanceRating\n",
      "0           1               3                  3\n",
      "1           2               2                  4\n",
      "2           3               3                  3\n",
      "3           4               2                  3\n",
      "4           5               3                  3 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4410 entries, 0 to 4409\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype\n",
      "---  ------             --------------  -----\n",
      " 0   EmployeeID         4410 non-null   int64\n",
      " 1   JobInvolvement     4410 non-null   int64\n",
      " 2   PerformanceRating  4410 non-null   int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 103.5 KB\n",
      "None \n",
      "\n",
      "=== In Time Data ===\n",
      "   Unnamed: 0  2015-01-01           2015-01-02           2015-01-05  \\\n",
      "0           1         NaN  2015-01-02 09:43:45  2015-01-05 10:08:48   \n",
      "1           2         NaN  2015-01-02 10:15:44  2015-01-05 10:21:05   \n",
      "2           3         NaN  2015-01-02 10:17:41  2015-01-05 09:50:50   \n",
      "3           4         NaN  2015-01-02 10:05:06  2015-01-05 09:56:32   \n",
      "4           5         NaN  2015-01-02 10:28:17  2015-01-05 09:49:58   \n",
      "\n",
      "            2015-01-06           2015-01-07           2015-01-08  \\\n",
      "0  2015-01-06 09:54:26  2015-01-07 09:34:31  2015-01-08 09:51:09   \n",
      "1                  NaN  2015-01-07 09:45:17  2015-01-08 10:09:04   \n",
      "2  2015-01-06 10:14:13  2015-01-07 09:47:27  2015-01-08 10:03:40   \n",
      "3  2015-01-06 10:11:07  2015-01-07 09:37:30  2015-01-08 10:02:08   \n",
      "4  2015-01-06 09:45:28  2015-01-07 09:49:37  2015-01-08 10:19:44   \n",
      "\n",
      "            2015-01-09           2015-01-12           2015-01-13  ...  \\\n",
      "0  2015-01-09 10:09:25  2015-01-12 09:42:53  2015-01-13 10:13:06  ...   \n",
      "1  2015-01-09 09:43:26  2015-01-12 10:00:07  2015-01-13 10:43:29  ...   \n",
      "2  2015-01-09 10:05:49  2015-01-12 10:03:47  2015-01-13 10:21:26  ...   \n",
      "3  2015-01-09 10:08:12  2015-01-12 10:13:42  2015-01-13 09:53:22  ...   \n",
      "4  2015-01-09 10:00:50  2015-01-12 10:29:27  2015-01-13 09:59:32  ...   \n",
      "\n",
      "            2015-12-18           2015-12-21           2015-12-22  \\\n",
      "0                  NaN  2015-12-21 09:55:29  2015-12-22 10:04:06   \n",
      "1  2015-12-18 10:37:17  2015-12-21 09:49:02  2015-12-22 10:33:51   \n",
      "2  2015-12-18 10:15:14  2015-12-21 10:10:28  2015-12-22 09:44:44   \n",
      "3  2015-12-18 10:17:38  2015-12-21 09:58:21  2015-12-22 10:04:25   \n",
      "4  2015-12-18 09:58:35  2015-12-21 10:03:41  2015-12-22 10:10:30   \n",
      "\n",
      "            2015-12-23           2015-12-24 2015-12-25           2015-12-28  \\\n",
      "0  2015-12-23 10:14:27  2015-12-24 10:11:35        NaN  2015-12-28 10:13:41   \n",
      "1  2015-12-23 10:12:10                  NaN        NaN  2015-12-28 09:31:45   \n",
      "2  2015-12-23 10:15:54  2015-12-24 10:07:26        NaN  2015-12-28 09:42:05   \n",
      "3  2015-12-23 10:11:46  2015-12-24 09:43:15        NaN  2015-12-28 09:52:44   \n",
      "4  2015-12-23 10:13:36  2015-12-24 09:44:24        NaN  2015-12-28 10:05:15   \n",
      "\n",
      "            2015-12-29           2015-12-30           2015-12-31  \n",
      "0  2015-12-29 10:03:36  2015-12-30 09:54:12  2015-12-31 10:12:44  \n",
      "1  2015-12-29 09:55:49  2015-12-30 10:32:25  2015-12-31 09:27:20  \n",
      "2  2015-12-29 09:43:36  2015-12-30 09:34:05  2015-12-31 10:28:39  \n",
      "3  2015-12-29 09:33:16  2015-12-30 10:18:12  2015-12-31 10:01:15  \n",
      "4  2015-12-29 10:30:53  2015-12-30 09:18:21  2015-12-31 09:41:09  \n",
      "\n",
      "[5 rows x 262 columns] \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4410 entries, 0 to 4409\n",
      "Columns: 262 entries, Unnamed: 0 to 2015-12-31\n",
      "dtypes: float64(12), int64(1), object(249)\n",
      "memory usage: 8.8+ MB\n",
      "None \n",
      "\n",
      "=== Out Time Data ===\n",
      "   Unnamed: 0  2015-01-01           2015-01-02           2015-01-05  \\\n",
      "0           1         NaN  2015-01-02 16:56:15  2015-01-05 17:20:11   \n",
      "1           2         NaN  2015-01-02 18:22:17  2015-01-05 17:48:22   \n",
      "2           3         NaN  2015-01-02 16:59:14  2015-01-05 17:06:46   \n",
      "3           4         NaN  2015-01-02 17:25:24  2015-01-05 17:14:03   \n",
      "4           5         NaN  2015-01-02 18:31:37  2015-01-05 17:49:15   \n",
      "\n",
      "            2015-01-06           2015-01-07           2015-01-08  \\\n",
      "0  2015-01-06 17:19:05  2015-01-07 16:34:55  2015-01-08 17:08:32   \n",
      "1                  NaN  2015-01-07 17:09:06  2015-01-08 17:34:04   \n",
      "2  2015-01-06 16:38:32  2015-01-07 16:33:21  2015-01-08 17:24:22   \n",
      "3  2015-01-06 17:07:42  2015-01-07 16:32:40  2015-01-08 16:53:11   \n",
      "4  2015-01-06 17:26:25  2015-01-07 17:37:59  2015-01-08 17:59:28   \n",
      "\n",
      "            2015-01-09           2015-01-12           2015-01-13  ...  \\\n",
      "0  2015-01-09 17:38:29  2015-01-12 16:58:39  2015-01-13 18:02:58  ...   \n",
      "1  2015-01-09 16:52:29  2015-01-12 17:36:48  2015-01-13 18:00:13  ...   \n",
      "2  2015-01-09 16:57:30  2015-01-12 17:28:54  2015-01-13 17:21:25  ...   \n",
      "3  2015-01-09 17:19:47  2015-01-12 17:13:37  2015-01-13 17:11:45  ...   \n",
      "4  2015-01-09 17:44:08  2015-01-12 18:51:21  2015-01-13 18:14:58  ...   \n",
      "\n",
      "            2015-12-18           2015-12-21           2015-12-22  \\\n",
      "0                  NaN  2015-12-21 17:15:50  2015-12-22 17:27:51   \n",
      "1  2015-12-18 18:31:28  2015-12-21 17:34:16  2015-12-22 18:16:35   \n",
      "2  2015-12-18 17:02:23  2015-12-21 17:20:17  2015-12-22 16:32:50   \n",
      "3  2015-12-18 17:55:23  2015-12-21 16:49:09  2015-12-22 17:24:00   \n",
      "4  2015-12-18 17:52:48  2015-12-21 17:43:35  2015-12-22 18:07:57   \n",
      "\n",
      "            2015-12-23           2015-12-24 2015-12-25           2015-12-28  \\\n",
      "0  2015-12-23 16:44:44  2015-12-24 17:47:22        NaN  2015-12-28 18:00:07   \n",
      "1  2015-12-23 17:38:18                  NaN        NaN  2015-12-28 17:08:38   \n",
      "2  2015-12-23 16:59:43  2015-12-24 16:58:25        NaN  2015-12-28 16:43:31   \n",
      "3  2015-12-23 17:36:35  2015-12-24 16:48:21        NaN  2015-12-28 17:19:34   \n",
      "4  2015-12-23 18:00:49  2015-12-24 17:59:22        NaN  2015-12-28 17:44:59   \n",
      "\n",
      "            2015-12-29           2015-12-30           2015-12-31  \n",
      "0  2015-12-29 17:22:30  2015-12-30 17:40:56  2015-12-31 17:17:33  \n",
      "1  2015-12-29 17:54:46  2015-12-30 18:31:35  2015-12-31 17:40:58  \n",
      "2  2015-12-29 17:09:56  2015-12-30 17:06:25  2015-12-31 17:15:50  \n",
      "3  2015-12-29 16:58:16  2015-12-30 17:40:11  2015-12-31 17:09:14  \n",
      "4  2015-12-29 18:47:00  2015-12-30 17:15:33  2015-12-31 17:42:14  \n",
      "\n",
      "[5 rows x 262 columns] \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4410 entries, 0 to 4409\n",
      "Columns: 262 entries, Unnamed: 0 to 2015-12-31\n",
      "dtypes: float64(12), int64(1), object(249)\n",
      "memory usage: 8.8+ MB\n",
      "None \n",
      "\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " # 4) Fusion des données",
   "id": "56a5842c9df8e5cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.1) Merge general_data, employee_survey_data et manager_survey_data",
   "id": "dc26c6ab46b3aff5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:10:36.291954Z",
     "start_time": "2025-01-27T08:10:36.287010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_merged = pd.merge(df_general, df_emp_survey, on=\"EmployeeID\")\n",
    "df_merged = pd.merge(df_merged, df_mgr_survey, on=\"EmployeeID\")"
   ],
   "id": "53b9d56da45676f1",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.2) Calculer le nombre d'heures travaillées par jour\n",
    "On va aussi créer des features à partir de df_in_time et df_out_time (par exemple : nombre d'heures travaillées moyennes). </br>\n",
    "Les colonnes de in_time / out_time sont des dates/heures pour chaque jour travaillé."
   ],
   "id": "70fdc0b02f93464a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:10:37.069114Z",
     "start_time": "2025-01-27T08:10:36.335474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#  - Pour chaque employé, on peut calculer la différence out_time - in_time (en heures) pour chaque jour.\n",
    "#  - Ensuite on peut faire la moyenne sur toutes les dates disponibles pour obtenir \"moyenne d'heures/jour\"\n",
    "\n",
    "# Replace Unnamed header by EmployeeID\n",
    "df_in_time.rename(columns={\"Unnamed: 0\": \"EmployeeID\"}, inplace=True)\n",
    "df_out_time.rename(columns={\"Unnamed: 0\": \"EmployeeID\"}, inplace=True)\n",
    "\n",
    "# Suppression de la première colonne \"EmployeeID\" pour faciliter les opérations (on la conserve à part)\n",
    "df_in_time_id = df_in_time['EmployeeID']\n",
    "df_out_time_id = df_out_time['EmployeeID']\n",
    "\n",
    "# On exclut la colonne 'EmployeeID' des dataframes pour ne traiter que les colonnes date/heure\n",
    "df_in_time_dates = df_in_time.drop(['EmployeeID'], axis=1)\n",
    "df_out_time_dates = df_out_time.drop(['EmployeeID'], axis=1)\n",
    "\n",
    "# Conversion des valeurs string en datetime pour permettre la soustraction des temps\n",
    "# Nota : Certaines valeurs sont \"NA\" => conversion en NaT\n",
    "df_in_time_dates = df_in_time_dates.apply(pd.to_datetime, errors='coerce')\n",
    "df_out_time_dates = df_out_time_dates.apply(pd.to_datetime, errors='coerce')\n",
    "\n",
    "# Calcul de la différence (out_time - in_time) par employé et par jour\n",
    "df_hours = df_out_time_dates - df_in_time_dates  # Résultat en format timedelta\n",
    "\n",
    "# Convertir les timedelta en nombre d'heures (float)\n",
    "df_hours = df_hours.apply(lambda x: x.dt.total_seconds() / 3600)\n",
    "\n",
    "# Exemple : Calcul d'une statistique agrégée (moyenne d'heures/jour travaillé) pour chaque employé\n",
    "df_hours['mean_work_hours'] = df_hours.mean(axis=1)\n",
    "\n",
    "# On peut aussi calculer le nombre de jours d'absence (journées entières manquantes => in_time = NA & out_time = NA)\n",
    "# ou le ratio de jours travaillés vs le total possible, etc.\n",
    "# Ci-dessous un exemple de calcul du nombre de jours (colonnes) pour lesquels l'entrée est manquante\n",
    "nb_jours_total = df_hours.shape[1] - 1  # -1 car la dernière colonne est 'mean_work_hours' qu'on vient d'ajouter\n",
    "df_hours['absent_days'] = df_hours.iloc[:, :-1].isna().sum(axis=1)  # On ne compte pas la col. 'mean_work_hours'\n",
    "\n",
    "# Concaténer EmployeeID pour pouvoir refusionner\n",
    "df_hours_final = pd.concat([df_in_time_id, df_hours[['mean_work_hours','absent_days']]], axis=1)"
   ],
   "id": "947e4ad54d41597e",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.3) Merge avec df_merged pour rajouter ces nouvelles features",
   "id": "dd78ce3782756295"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:10:37.119387Z",
     "start_time": "2025-01-27T08:10:37.107426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_merged = pd.merge(df_merged, df_hours_final, on='EmployeeID', how='left')\n",
    "\n",
    "print(\"\\n=== Aperçu des données fusionnées ===\\n\")\n",
    "print(df_merged.head())\n",
    "print(df_merged.info())"
   ],
   "id": "1c4b1a2d849d0160",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Aperçu des données fusionnées ===\n",
      "\n",
      "   Age Attrition     BusinessTravel              Department  DistanceFromHome  \\\n",
      "0   51        No      Travel_Rarely                   Sales                 6   \n",
      "1   31       Yes  Travel_Frequently  Research & Development                10   \n",
      "2   32        No  Travel_Frequently  Research & Development                17   \n",
      "3   38        No         Non-Travel  Research & Development                 2   \n",
      "4   32        No      Travel_Rarely  Research & Development                10   \n",
      "\n",
      "   Education EducationField  EmployeeCount  EmployeeID  Gender  ...  \\\n",
      "0          2  Life Sciences              1           1  Female  ...   \n",
      "1          1  Life Sciences              1           2  Female  ...   \n",
      "2          4          Other              1           3    Male  ...   \n",
      "3          5  Life Sciences              1           4    Male  ...   \n",
      "4          1        Medical              1           5    Male  ...   \n",
      "\n",
      "   YearsAtCompany YearsSinceLastPromotion YearsWithCurrManager  \\\n",
      "0               1                       0                    0   \n",
      "1               5                       1                    4   \n",
      "2               5                       0                    3   \n",
      "3               8                       7                    5   \n",
      "4               6                       0                    4   \n",
      "\n",
      "   EnvironmentSatisfaction  JobSatisfaction WorkLifeBalance  JobInvolvement  \\\n",
      "0                      3.0              4.0             2.0               3   \n",
      "1                      3.0              2.0             4.0               2   \n",
      "2                      2.0              2.0             1.0               3   \n",
      "3                      4.0              4.0             3.0               2   \n",
      "4                      4.0              1.0             3.0               3   \n",
      "\n",
      "   PerformanceRating  mean_work_hours  absent_days  \n",
      "0                  3         7.373651           29  \n",
      "1                  4         7.718969           25  \n",
      "2                  3         7.013240           19  \n",
      "3                  3         7.193678           26  \n",
      "4                  3         8.006175           16  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4410 entries, 0 to 4409\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Age                      4410 non-null   int64  \n",
      " 1   Attrition                4410 non-null   object \n",
      " 2   BusinessTravel           4410 non-null   object \n",
      " 3   Department               4410 non-null   object \n",
      " 4   DistanceFromHome         4410 non-null   int64  \n",
      " 5   Education                4410 non-null   int64  \n",
      " 6   EducationField           4410 non-null   object \n",
      " 7   EmployeeCount            4410 non-null   int64  \n",
      " 8   EmployeeID               4410 non-null   int64  \n",
      " 9   Gender                   4410 non-null   object \n",
      " 10  JobLevel                 4410 non-null   int64  \n",
      " 11  JobRole                  4410 non-null   object \n",
      " 12  MaritalStatus            4410 non-null   object \n",
      " 13  MonthlyIncome            4410 non-null   int64  \n",
      " 14  NumCompaniesWorked       4391 non-null   float64\n",
      " 15  Over18                   4410 non-null   object \n",
      " 16  PercentSalaryHike        4410 non-null   int64  \n",
      " 17  StandardHours            4410 non-null   int64  \n",
      " 18  StockOptionLevel         4410 non-null   int64  \n",
      " 19  TotalWorkingYears        4401 non-null   float64\n",
      " 20  TrainingTimesLastYear    4410 non-null   int64  \n",
      " 21  YearsAtCompany           4410 non-null   int64  \n",
      " 22  YearsSinceLastPromotion  4410 non-null   int64  \n",
      " 23  YearsWithCurrManager     4410 non-null   int64  \n",
      " 24  EnvironmentSatisfaction  4385 non-null   float64\n",
      " 25  JobSatisfaction          4390 non-null   float64\n",
      " 26  WorkLifeBalance          4372 non-null   float64\n",
      " 27  JobInvolvement           4410 non-null   int64  \n",
      " 28  PerformanceRating        4410 non-null   int64  \n",
      " 29  mean_work_hours          4410 non-null   float64\n",
      " 30  absent_days              4410 non-null   int64  \n",
      "dtypes: float64(6), int64(17), object(8)\n",
      "memory usage: 1.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5) Nettoyage et préparation des données",
   "id": "2820c7d5605eed20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5.1) Gérer les valeurs manquantes",
   "id": "5a9b6e778e5377c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:10:37.164362Z",
     "start_time": "2025-01-27T08:10:37.159426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# On regarde déjà combien de valeurs manquantes par colonne\n",
    "missing_values = df_merged.isnull().sum()\n",
    "print(\"\\n=== Nombre de valeurs manquantes par colonne ===\\n\", missing_values)"
   ],
   "id": "7bb686111f61c3ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Nombre de valeurs manquantes par colonne ===\n",
      " Age                         0\n",
      "Attrition                   0\n",
      "BusinessTravel              0\n",
      "Department                  0\n",
      "DistanceFromHome            0\n",
      "Education                   0\n",
      "EducationField              0\n",
      "EmployeeCount               0\n",
      "EmployeeID                  0\n",
      "Gender                      0\n",
      "JobLevel                    0\n",
      "JobRole                     0\n",
      "MaritalStatus               0\n",
      "MonthlyIncome               0\n",
      "NumCompaniesWorked         19\n",
      "Over18                      0\n",
      "PercentSalaryHike           0\n",
      "StandardHours               0\n",
      "StockOptionLevel            0\n",
      "TotalWorkingYears           9\n",
      "TrainingTimesLastYear       0\n",
      "YearsAtCompany              0\n",
      "YearsSinceLastPromotion     0\n",
      "YearsWithCurrManager        0\n",
      "EnvironmentSatisfaction    25\n",
      "JobSatisfaction            20\n",
      "WorkLifeBalance            38\n",
      "JobInvolvement              0\n",
      "PerformanceRating           0\n",
      "mean_work_hours             0\n",
      "absent_days                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:10:37.213332Z",
     "start_time": "2025-01-27T08:10:37.209160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dropna sur la colonne Attrition car c'est notre target\n",
    "df_merged = df_merged.dropna(subset=['Attrition'])"
   ],
   "id": "36fd2752b3b6e64b",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:16:08.317489Z",
     "start_time": "2025-01-27T08:16:08.313940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Exemple d'imputation : pour 'mean_work_hours' et 'absent_days', on remplace les NaN par la moyenne\n",
    "# On privilégie la moyenne ici car les valeurs de type durée (heures) ne sont pas fortement bornées\n",
    "df_merged['mean_work_hours'] = df_merged['mean_work_hours'].fillna(df_merged['mean_work_hours'].mean())\n",
    "print(f\"Filled missing values for column 'mean_work_hours' with mean value {df_merged['mean_work_hours'].mean()}\")\n",
    "df_merged['absent_days'] = df_merged['absent_days'].fillna(df_merged['absent_days'].mean())\n",
    "print(f\"Filled missing values for column 'absent_days' with mean value {df_merged['absent_days'].mean()}\")"
   ],
   "id": "ca2427042f08cf3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled missing values for column 'mean_work_hours' with mean value 7.700791784478147\n",
      "Filled missing values for column 'absent_days' with mean value 24.73469387755102\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:16:47.994536Z",
     "start_time": "2025-01-27T08:16:47.988037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pour les colonnes de satisfaction ou autres colonnes numériques manquantes, on peut aussi faire un fillna\n",
    "# On privilégie la médiane\n",
    "num_cols = ['EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', \n",
    "            'JobInvolvement', 'PerformanceRating', 'TotalWorkingYears', 'NumCompaniesWorked',]\n",
    "for col in num_cols:\n",
    "    if col in df_merged.columns:\n",
    "        df_merged[col] = df_merged[col].fillna(df_merged[col].median())\n",
    "        print(f\"Filled missing values for column {col} with median value {df_merged[col].median()}\")"
   ],
   "id": "3bcb7a7cbf3faa1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled missing values for column EnvironmentSatisfaction with median value 3.0\n",
      "Filled missing values for column JobSatisfaction with median value 3.0\n",
      "Filled missing values for column WorkLifeBalance with median value 3.0\n",
      "Filled missing values for column JobInvolvement with median value 3.0\n",
      "Filled missing values for column PerformanceRating with median value 3.0\n",
      "Filled missing values for column TotalWorkingYears with median value 10.0\n",
      "Filled missing values for column NumCompaniesWorked with median value 2.0\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5.2) Nettoyage de certaines colonnes (ex: Over18, EmployeeCount, StandardHours)",
   "id": "f1cf019fe192eb53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:10:37.356560Z",
     "start_time": "2025-01-27T08:10:37.351047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# On remarque souvent dans ce dataset \"EmployeeCount\" est toujours 1 => pas d'intérêt\n",
    "# \"Over18\" est toujours \"Y\", \"StandardHours\" est souvent 8 => on peut les drop\n",
    "cols_to_drop = ['Over18','StandardHours','EmployeeCount'] \n",
    "for c in cols_to_drop:\n",
    "    if c in df_merged.columns:\n",
    "        df_merged.drop(c, axis=1, inplace=True, errors='ignore')"
   ],
   "id": "cd8762219021a0a5",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5.3) Conversion de colonnes catégorielles en numériques",
   "id": "1c95da9bd6241297"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:10:39.629382Z",
     "start_time": "2025-01-27T08:10:39.594169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Par exemple, Attrition = Yes/No, Gender = Male/Female, etc.\n",
    "# On peut les encoder, soit via LabelEncoder, soit via OneHotEncoder\n",
    "# Commençons par un label encoding simple pour la variable cible\n",
    "\n",
    "df_merged['Attrition'] = df_merged['Attrition'].map({'Yes':1, 'No':0})\n",
    "\n",
    "# Autres colonnes catégorielles (BusinessTravel, Department, EducationField, Gender, MaritalStatus, JobRole...)\n",
    "cat_cols = ['BusinessTravel','Department','EducationField','Gender','MaritalStatus','JobRole']\n",
    "\n",
    "# On va faire un one-hot-encoding rapide:\n",
    "df_merged = pd.get_dummies(df_merged, columns=cat_cols, drop_first=True)"
   ],
   "id": "d947248ca8f41304",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['BusinessTravel', 'Department', 'EducationField', 'Gender',\\n       'MaritalStatus', 'JobRole'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[54], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m cat_cols \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBusinessTravel\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDepartment\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEducationField\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGender\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMaritalStatus\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mJobRole\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# On va faire un one-hot-encoding rapide:\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m df_merged \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_dummies\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_merged\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcat_cols\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdrop_first\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/AttritionPrediction/.venv/lib/python3.12/site-packages/pandas/core/reshape/encoding.py:169\u001B[0m, in \u001B[0;36mget_dummies\u001B[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001B[0m\n\u001B[1;32m    167\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput must be a list-like for parameter `columns`\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 169\u001B[0m     data_to_encode \u001B[38;5;241m=\u001B[39m \u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# validate prefixes and separator to avoid silently dropping cols\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcheck_len\u001B[39m(item, name: \u001B[38;5;28mstr\u001B[39m):\n",
      "File \u001B[0;32m~/PycharmProjects/AttritionPrediction/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   4106\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n\u001B[1;32m   4107\u001B[0m         key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[0;32m-> 4108\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_indexer_strict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcolumns\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m   4110\u001B[0m \u001B[38;5;66;03m# take() does not accept boolean indexers\u001B[39;00m\n\u001B[1;32m   4111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(indexer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/AttritionPrediction/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001B[0m, in \u001B[0;36mIndex._get_indexer_strict\u001B[0;34m(self, key, axis_name)\u001B[0m\n\u001B[1;32m   6197\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   6198\u001B[0m     keyarr, indexer, new_indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reindex_non_unique(keyarr)\n\u001B[0;32m-> 6200\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raise_if_missing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeyarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   6202\u001B[0m keyarr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtake(indexer)\n\u001B[1;32m   6203\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Index):\n\u001B[1;32m   6204\u001B[0m     \u001B[38;5;66;03m# GH 42790 - Preserve name from an Index\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/AttritionPrediction/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6249\u001B[0m, in \u001B[0;36mIndex._raise_if_missing\u001B[0;34m(self, key, indexer, axis_name)\u001B[0m\n\u001B[1;32m   6247\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m nmissing:\n\u001B[1;32m   6248\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m nmissing \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(indexer):\n\u001B[0;32m-> 6249\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNone of [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] are in the [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maxis_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   6251\u001B[0m     not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(ensure_index(key)[missing_mask\u001B[38;5;241m.\u001B[39mnonzero()[\u001B[38;5;241m0\u001B[39m]]\u001B[38;5;241m.\u001B[39munique())\n\u001B[1;32m   6252\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnot_found\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in index\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyError\u001B[0m: \"None of [Index(['BusinessTravel', 'Department', 'EducationField', 'Gender',\\n       'MaritalStatus', 'JobRole'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6) Séparation des données en train/test",
   "id": "15e665b9971bbec1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:07:36.860961Z",
     "start_time": "2025-01-27T08:07:36.853984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# On sépare la cible (Attrition) des features\n",
    "\n",
    "X = df_merged.drop(['EmployeeID','Attrition'], axis=1)\n",
    "y = df_merged['Attrition']\n",
    "\n",
    "# Ensuite on fait un split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")"
   ],
   "id": "b75e224a476a1de4",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 7) scaling et entraînement d'un modèle simple",
   "id": "ba440a38f1d808be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:07:38.515707Z",
     "start_time": "2025-01-27T08:07:38.501144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Selon le modèle (Logistic Regression par exemple), il peut être intéressant de standardiser\n",
    "# Ici on va montrer un exemple de pipeline manuel (scaling + logistic regression).\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "id": "4f5d93c6722c17d0",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7.1) Entrainement d'un modèle de Logistic Regression",
   "id": "b24bf4ba78ad12e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T08:09:58.388546Z",
     "start_time": "2025-01-27T08:09:58.384246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#clf_lr = LogisticRegression(random_state=42, max_iter=500)\n",
    "#clf_lr.fit(X_train_scaled, y_train)"
   ],
   "id": "b4402e4358f0af3f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                                   0\n",
       "DistanceFromHome                      0\n",
       "Education                             0\n",
       "JobLevel                              0\n",
       "MonthlyIncome                         0\n",
       "NumCompaniesWorked                   15\n",
       "PercentSalaryHike                     0\n",
       "StockOptionLevel                      0\n",
       "TotalWorkingYears                     0\n",
       "TrainingTimesLastYear                 0\n",
       "YearsAtCompany                        0\n",
       "YearsSinceLastPromotion               0\n",
       "YearsWithCurrManager                  0\n",
       "EnvironmentSatisfaction               0\n",
       "JobSatisfaction                       0\n",
       "WorkLifeBalance                       0\n",
       "JobInvolvement                        0\n",
       "PerformanceRating                     0\n",
       "mean_work_hours                       0\n",
       "absent_days                           0\n",
       "BusinessTravel_Travel_Frequently      0\n",
       "BusinessTravel_Travel_Rarely          0\n",
       "Department_Research & Development     0\n",
       "Department_Sales                      0\n",
       "EducationField_Life Sciences          0\n",
       "EducationField_Marketing              0\n",
       "EducationField_Medical                0\n",
       "EducationField_Other                  0\n",
       "EducationField_Technical Degree       0\n",
       "Gender_Male                           0\n",
       "MaritalStatus_Married                 0\n",
       "MaritalStatus_Single                  0\n",
       "JobRole_Human Resources               0\n",
       "JobRole_Laboratory Technician         0\n",
       "JobRole_Manager                       0\n",
       "JobRole_Manufacturing Director        0\n",
       "JobRole_Research Director             0\n",
       "JobRole_Research Scientist            0\n",
       "JobRole_Sales Executive               0\n",
       "JobRole_Sales Representative          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
